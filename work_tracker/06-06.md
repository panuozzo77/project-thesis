
|                  | Inizio | Fine | Inizio | Fine |        | Totale |
| ---------------- | ------ | ---- | ------ | ---- | ------ | ------ |
| Orari Lavorativi | 9      | 13   | 14     | 16   | Totale | 6      |
## Lavoro Svolto
- Refactoring di tutto il codice per una miglior leggibilità. Il codice era diventato troppo pesante e difficile da seguire siccome stavo eseguendo parecchi test sugli approcci da utilizzare per eseguire le operazioni, risultando spaghettificato.

Al momento, l'applicativo:
- **Organizzazione dei percorsi:** Stabilisce la cartella principale del progetto e gestisce tutti i percorsi importanti (come le cartelle dei dati) tramite un registro centralizzato (`PathRegistry`).
- **Configurazione principale:** Carica le impostazioni principali da un file chiamato `config.json`. Se non esiste, ne crea uno predefinito con un modello utile. Puoi anche specificare un file di configurazione personalizzato all'avvio.
- **Logger unico:** Utilizza un sistema di logging centralizzato (`LoggerManager`) per registrare i messaggi in tutta l'applicazione.
	- **Log pre-avvio:** Ha un "logger" speciale per i messaggi iniziali, prima che la configurazione principale sia completamente caricata.
	- **Configurazione personalizzata:** Il logger principale viene configurato (nome, livello di dettaglio, formato dei messaggi, e la possibilità di salvare i log in file che ruotano) in base alle impostazioni nel `config.json`.
- **Aiuto automatico:** Se scrivi `python run.py --help`, l'applicazione ti mostrerà un messaggio di aiuto con tutti i comandi disponibili.

Comandi via CLI:
**Definizione dei comandi:** I comandi disponibili sono definiti in modo chiaro. Attualmente puoi usare:

- `--config`: per specificare un file di configurazione diverso.
- `--load_etl`: per eseguire tutti i processi ETL elencati nel tuo `config.json`

**Esecuzione dei comandi:** L'applicazione interpreta i comandi che inserisci e attiva le azioni corrispondenti.

- **Cosa definisce un file ETL:** Ogni file ETL specifica:
    - Il **database MongoDB** di destinazione.
    - Un elenco di **file sorgente** (CSV o JSON) da processare per diverse "collezioni" (tabelle in MongoDB).
    - Per ogni file sorgente:
        - Il nome della **collezione MongoDB** di destinazione.
        - Le **regole di mappatura** per rinominare le colonne/chiavi dei file sorgente nei campi dei documenti MongoDB.
        - Le **regole di conversione del tipo di dato** (testo, numeri interi, numeri decimali, valori veri/falsi).
        - Opzioni base per i file CSV (come il separatore di colonne).
- **Flusso di lavoro:** Carica i dati dai file, li trasforma secondo le regole di mappatura (rinominando colonne, convertendo tipi) e infine li carica nel database e nelle collezioni MongoDB specificate.

```json
# file etl.json
// qui si definisce la struttura delle informazioni presenti sul file come vanno caricate
{  
  "collections": [  
    {  
      "file": "test.json", // This is a placeholder file name, replace with actual file.  
      "collection": "book", // This is the name of the collection in the database.  
      "mapping": {  
        "<SourceField>": { "<RenamedFieldOnDB>": "title", "type": "str" }, // Replace <SourceField> with the actual field name from the source data.  
        "AnnoPubblicazione": { "field": "year", "type": "int" }, // Examples here  
        "Pagine": { "field": "pages", "type": "int" },  
        "Autore": { "field": "author", "type": "str" }  
      }  
    }  
  ]  
}
```

```json
# file config.json
{  
    "project_name": "GoodreadsRecommender",  
    "version": "0.1.0",  
    "author": "Your Name/Group",  
    "logging": {  
        "name": "ProjectLogger",  
        "level": "DEBUG",  
        "format": "%(asctime)s - %(levelname)s - [%(module)s.%(funcName)s:%(lineno)d] - %(message)s"  
    },  
    "database": {  
        "type": "mongodb",  
        "uri": "mongodb://localhost:27017/",  
        "db_name": "goodreads_recommender"  
    },  
    "data_paths": {  
        "raw_datasets_dir": "downloaded_datasets/partial/",  
        "etl_configs_dir": "etl_configurations/"  
    },  
    "etl_list": [  
        "etl.json"  //qui si definiscono tutti i file da caricare sul db
    ]  
}
```


--- 
Stato dell'arte:
**Lunedì 3 Giugno:**

- Hai iniziato a stendere i documenti RAD (Requirements Analysis Document) e SDD (Software Design Document) in forma sintetica.
- Hai creato un repository GitHub e scelto la licenza MIT per il progetto.
- Sono state generate 4 macro-issue basate sui requisiti funzionali principali, che verranno specializzate in seguito.
- Per lo sviluppo, seguirai la metodologia Git Flow, usando branch dedicati alle funzionalità e integrando nel branch principale quando le funzionalità saranno complete o raffinate a sufficienza.
- Hai migliorato la gestione delle Daily Notes per etichettare meglio il lavoro svolto.
- È stato aggiunto il file di registro delle ore di tirocinio al `.gitignore` in quanto contiene dati sensibili.

**Martedì 4 Giugno:**

- Hai lavorato sulla parte del software (`ArgumentHandler`) che permetterà di controllare le operazioni tramite argomenti da riga di comando.
- Hai impostato la connessione al database.
- Hai iniziato a lavorare sulla trasformazione dei dataset in formato JSON e CSV in tabelle di database equivalenti. Questo include la gestione di informazioni mancanti o la trasformazione di dati (ad esempio, unificare date presenti in tre colonne separate).
- La definizione di questa manipolazione dei dataset avviene tramite file di configurazione `.json` che l'utente (curatore) può definire.

**Mercoledì 5 Giugno:**

- Hai prodotto un'applicazione che consente di specificare quali campi inviare al database senza trasformazioni, eseguendo quindi solo l'estrazione delle funzionalità.
- Anche in questo caso, la manipolazione dei dataset è definita tramite file di configurazione `.json` che l'utente può personalizzare.

**Giovedì 6 Giugno:**

- Hai eseguito un refactoring completo del codice per migliorarne la leggibilità, dato che era diventato troppo complesso e difficile da seguire a causa dei test sui vari approcci.

**Allo stato attuale, l'applicativo svolge le seguenti funzioni:**

- **Organizzazione dei percorsi:** Stabilisce la cartella principale del progetto e gestisce i percorsi importanti (come le cartelle dei dati) tramite un registro centralizzato (`PathRegistry`).
- **Configurazione principale:** Carica le impostazioni principali da un file `config.json`. Se non esiste, ne crea uno predefinito. È possibile specificare un file di configurazione personalizzato all'avvio.
- **Logging centralizzato:** Utilizza un sistema di logging centralizzato (`LoggerManager`) per registrare i messaggi.
    - Ha un logger speciale per i messaggi iniziali prima del caricamento della configurazione principale.
    - Il logger principale è configurato (nome, livello, formato, output su file rotanti) in base alle impostazioni di `config.json`.
- **Aiuto automatico:** Se si esegue `python run.py --help`, l'applicazione mostra un messaggio di aiuto con i comandi disponibili.

**Comandi disponibili tramite CLI:**

- `--config`: per specificare un file di configurazione diverso.
- `--load_etl`: per eseguire tutti i processi ETL elencati nel `config.json`.
- L'applicazione interpreta i comandi inseriti e attiva le azioni corrispondenti.

**Funzionalità ETL:**

- Ogni file ETL (`etl.json` ad esempio) definisce il database MongoDB di destinazione e un elenco di file sorgente (CSV o JSON) da processare per diverse collezioni.
- Per ogni file sorgente, specifica il nome della collezione MongoDB di destinazione, le regole di mappatura (rinominare colonne/chiavi) e le regole di conversione del tipo di dato (stringa, intero, float, booleano). Sono incluse anche opzioni base per i file CSV (come il separatore).
- Il flusso di lavoro prevede il caricamento dei dati, la trasformazione secondo le regole di mappatura e il caricamento nel database e nelle collezioni MongoDB specificate.

---
- I prossimi passi includono la produzione di tutti i parsing necessari per importare strutture come liste e dizionari, non solo stringhe, interi, float e booleani.
    
- **Flusso di lavoro:** Carica i dati dai file, li trasforma secondo le regole di mappatura (rinominando colonne, convertendo tipi) e infine li carica nel database e nelle collezioni MongoDB specificate. Questo è necessario per la **feature extracation**
    
- **Data augmentation:** Successivamente, procederà alla fase di data augmentation.